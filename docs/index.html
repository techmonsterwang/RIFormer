<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>RIFormer</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <div class="title", style="padding-top: 35pt;min-width: 200px;">  <!-- Set padding as 10 if title is with two lines. -->
      RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer
    </div>
   <!-- <h2 style="font-size:20px;text-align:center;"> ArXiv 2023 <h2> -->
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://scholar.google.com/citations?user=QjVR3UUAAAAJ&hl=zh-CN" target="_blank">Jiahao Wang</a><sup>1,2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="http://www.zhangsongyang.com/" target="_blank">Songyang Zhang</a><sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://workforai.github.io/" target="_blank">Yong Liu</a><sup>3</sup> &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://wutaiqiang.github.io/" target="_blank">Taiqiang Wu</a><sup>3</sup> &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://sites.google.com/view/iigroup-thu" target="_blank">Yujiu Yang</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://xh-liu.github.io/" target="_blank">Xihui Liu</a><sup>2</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://chenkai.site/" target="_blank">Kai Chen</a><sup>1</sup> &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="http://luoping.me/" target="_blank">Ping Luo</a><sup>2</sup> &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="http://dahua.site/" target="_blank">Dahua Lin</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
  </div>
  <div class="institution">
    <sup>1</sup> Shanghai AI Laboratory&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>2</sup> The University of Hong Kong&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>3</sup> Tsinghua University&nbsp;&nbsp;&nbsp;&nbsp;<br>
  </div>
  <div class="link">
    <a href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://github.com/open-mmlab/mmpretrain/tree/main/configs/riformer" target="_blank">[Code and model (mmpretrain)]</a>&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://github.com/techmonsterwang/RIFormer" target="_blank">[Code and model (timm)]</a>

  </div>
  <div class="module_imitation">
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/module_imitation.png" width="95%"></td>
      </tr>
    </table>
  </div>
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Abstract</div>
  <div class="body">
    This paper studies how to keep a vision backbone effective while removing token mixers in its basic building blocks. Token mixers, as self-attention for vision transformers (ViTs), are intended to perform information communication between different spatial tokens but suffer from considerable computational cost and latency. However, directly removing them will lead to an incomplete model structure prior, and thus brings a significant accuracy drop. To this end, we first develop an RepIdentityFormer (RIFormer) base on the re-parameterizing idea, to study the token mixer free model architecture. And we then explore the improved learning paradigm to break the limitation of simple token mixer free backbone, and summarize the empirical practice into <b>5 guidelines</b>. Equipped with the proposed optimization strategy, we are able to build an extremely simple vision backbone with encouraging performance, while enjoying the high efficiency during inference. Extensive experiments and ablative analysis also demonstrate that the inductive bias of network architecture, can be incorporated into simple network structure with appropriate optimization strategy. We hope this work can serve as a starting point for the exploration of <b>optimization-driven efficient network design</b>.
    
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/reparameterization.png" width="68%"></td>
      </tr>
    </table>
  </div>

</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Results</div>
  <div class="body">

    Results of models with different types of token mixers on ImageNet-1K. ⋄ denotes training with prolonged 600 epochs. ‡ denotes fine-tuning from the ImageNet pre-trained model for 30 epochs. Notably, without token mixer, RIFormer cannot even perform basic token mixing operation in its building blocks. However, the ImageNet experiments demonstrate that with the proposed training paradigm, RIFormer still shows promising results. We can only deem the reason behind the fact might be that optimization strategy plays a key role. RIFormer is readily a starting recipe for the exploration of optimization-driven efficient network design, and rest assured of the performance with advanced training schemes.


    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/imagenet.png" width="88%"></td>
      </tr>
    </table>

    Visualization of the feature distribution of the first and last stage of PoolFormer-S12 and RIFormer-S12.
    After applying the proposed module imitation, the distribution of RIFormer-S12 are basically shifted toward that of the PoolFomer-S12, demonstrating its effect on helping student learn useful knowledge from the teacher


    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/feature.png" width="78%"></td>
      </tr>
    </table>

  </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
@article{wang2023riformer,
  title   = {RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer},
  author  = {Wang, Jiahao and Zhang, Songyang and Liu, Yong and Wu, Taiqiang and Yang, Yujiu and Liu, Xihui and Chen, Kai and Luo, Ping and Lin, Dahua},
  journal = {arXiv preprint arXiv:xxxx.xxxxx},
  year    = {2023}
}
</pre>

  <!-- BZ: we should give other related work enough credits, -->
  <!--     so please include some most relevant work and leave some comment to summarize work and the difference. -->
  <div class="ref">Related Work</div>

  <div class="citation">
    <div class="image"><img src="./assets/poolformer.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/abs/2111.11418" target="_blank">
        Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, Shuicheng Yan.
        MetaFormer Is Actually What You Need for Vision.
        CVPR 2023.</a><br>
      <b>Comment:</b>
      Hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance.
    </div>
  </div>

  <div class="citation">
    <div class="image"><img src="./assets/metaformer.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/abs/2210.13452" target="_blank">
        Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, Xinchao Wang.
        MetaFormer Baselines for Vision.
        Arxiv 2023.</a><br>
      <b>Comment:</b>
      Introduce several baseline models under MetaFormer using the most basic or common mixers, and demonstrate their gratifying performance.
    </div>
  </div>
  
</div>
<!-- === Reference Section Ends === -->

</body>
</html>
